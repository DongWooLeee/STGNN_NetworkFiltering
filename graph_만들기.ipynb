{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import planarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "# load the data\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # 영업일에만 존재하는 데이터\n",
    "    df_open = pd.read_csv('open.csv',index_col=0, thousands=',')\n",
    "    df_open.index = pd.to_datetime(df_open.index)\n",
    "    df_high = pd.read_csv('high.csv',index_col=0, thousands=',')\n",
    "    df_high.index = pd.to_datetime(df_high.index)\n",
    "    df_low = pd.read_csv('low.csv',index_col=0, thousands=',')\n",
    "    df_low.index = pd.to_datetime(df_low.index)\n",
    "    df_close = pd.read_csv('close.csv',index_col=0, thousands=',')\n",
    "    df_close.index = pd.to_datetime(df_close.index)\n",
    "    df_vol = pd.read_csv('volume.csv',index_col=0, thousands=',')\n",
    "    df_vol.index = pd.to_datetime(df_vol.index)\n",
    "    # 아래 시가총액 데이터는 매일 존재함\n",
    "    df_markets = pd.read_csv('market_equity_kospi.csv',index_col=0) #이는 매일 존재\n",
    "   \n",
    "    return df_open, df_high, df_low, df_close, df_vol, df_markets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# downgrade pytorch to 2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 자산 풀은 30개 정도로 가져가보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch-scatter  version 확인\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_open, df_high, df_low, df_close, df_vol, df_markets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [00:00<00:00, 6489.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#시가총액 기준으로 상위 50개 뽑기(동적인 자산풀 운영)\n",
    "selected_stocks = df_markets.loc['2020-01-31'].sort_values(ascending=False)[:50].index \n",
    "\n",
    "rebal_date = pd.to_datetime('2020-01-31')\n",
    "rebal_date_index = df_close.index.get_loc(rebal_date)\n",
    "valid_stocks = []\n",
    "for stock in selected_stocks:\n",
    "    if (df_open[stock].iloc[rebal_date_index-19:rebal_date_index+1].notnull().all() and\n",
    "        df_high[stock].iloc[rebal_date_index-19:rebal_date_index+1].notnull().all() and\n",
    "        df_low[stock].iloc[rebal_date_index-19:rebal_date_index+1].notnull().all() and\n",
    "        df_close[stock].iloc[rebal_date_index-19:rebal_date_index+1].notnull().all() and\n",
    "        df_vol[stock].iloc[rebal_date_index-19:rebal_date_index+1].notnull().all()):\n",
    "        valid_stocks.append(stock)\n",
    "\n",
    "# 다시 재정렬하고 30개를 뽑아서 selected_stocks_final을 만들기\n",
    "\n",
    "selected_stocks_final = df_markets.loc['2020-01-31', valid_stocks].sort_values(ascending=False)[:30].index.tolist()\n",
    "\n",
    "#2020년부터 2022년까지 데이터 사용\n",
    "df_open_temp = df_open['2020-01-01':'2022-01-01']\n",
    "df_high_temp = df_high['2020-01-01':'2022-01-01']\n",
    "df_low_temp = df_low['2020-01-01':'2022-01-01']\n",
    "df_close_temp = df_close['2020-01-01':'2022-01-01']\n",
    "df_vol_temp = df_vol['2020-01-01':'2022-01-01']\n",
    "# length는 다를 수 있음!!\n",
    "df_markets = df_markets['2020-01-01':'2022-01-01']\n",
    "#Tensor로 한꺼번에 구축하는 것도 방법-> 차후 고민\n",
    "temp_df = pd.DataFrame(index=selected_stocks, columns = ['open', 'high', 'low', 'close', 'volume']) # 30*5 형태. 각 날짜별로 이는 구축됨.\n",
    "\n",
    "open_values = df_open_temp.loc[rebal_date, selected_stocks]\n",
    "high_values = df_high_temp.loc[rebal_date, selected_stocks]\n",
    "low_values = df_low_temp.loc[rebal_date, selected_stocks]\n",
    "close_values = df_close_temp.loc[rebal_date, selected_stocks]\n",
    "vol_values = df_vol_temp.loc[rebal_date, selected_stocks]\n",
    "\n",
    "temp_df['open'] = open_values.values\n",
    "temp_df['high'] = high_values.values\n",
    "temp_df['low'] = low_values.values\n",
    "temp_df['close'] = close_values.values\n",
    "temp_df['volume'] = vol_values.values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corr_df = df_close.iloc[rebal_date_index-19:rebal_date_index+1,:] #인덱싱 주의, 종가 기준으로 상관계수 matrix 생성\n",
    "corr_df = corr_df[selected_stocks_final]\n",
    "corrs = corr_df.corr()\n",
    "corr_matrix = np.array(corrs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_network_PMFG(corr_matrix):\n",
    "\n",
    "    # Get the list of decreasing weighted links\n",
    "    rholist = []\n",
    "    n = len(corr_matrix)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i < j and corr_matrix[i][j] != 0:\n",
    "                rholist.append([abs(float(corr_matrix[i][j])), i, j])\n",
    "\n",
    "    rholist.sort(key=lambda x: x[0])\n",
    "    rholist.reverse()\n",
    "\n",
    "    m = len(rholist)\n",
    "    filtered_matr = np.zeros((n, n))\n",
    "    control = 0\n",
    "\n",
    "    # Use tqdm for progress display\n",
    "    for t in tqdm(range(m)):\n",
    "        if control <= 3 * (n - 2) - 1:\n",
    "            i = rholist[t][1]\n",
    "            j = rholist[t][2]\n",
    "            filtered_matr[i][j] = rholist[t][0]\n",
    "\n",
    "            # Check planarity here\n",
    "            G = nx.Graph()\n",
    "            for k in range(n):\n",
    "                for l in range(n):\n",
    "                    if filtered_matr[k][l] != 0:\n",
    "                        G.add_edge(k, l, weight=filtered_matr[k][l])\n",
    "\n",
    "            if not planarity.is_planar(G):\n",
    "                filtered_matr[i][j] = 0\n",
    "                control += 1\n",
    "\n",
    "    # Build the PMFG network\n",
    "    PMFG = nx.Graph()\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if filtered_matr[i][j] != 0:\n",
    "                PMFG.add_edge(i, j, weight=filtered_matr[i][j])\n",
    "\n",
    "    return PMFG\n",
    "\n",
    "#turn it as a torch_geometric graph\n",
    "G = get_network_PMFG(corr_matrix)\n",
    "\n",
    "# 그래프 feature 만들어 주기\n",
    "for node in G.nodes:\n",
    "    open_price = temp_df.iloc[node,0] #open\n",
    "    G.nodes[node]['open'] = open_price\n",
    "    high_price = temp_df.iloc[node,1] #high\n",
    "    G.nodes[node]['high'] = high_price\n",
    "    low_price = temp_df.iloc[node,2] #low\n",
    "    G.nodes[node]['low'] = low_price\n",
    "    close_price = temp_df.iloc[node,3] #close\n",
    "    G.nodes[node]['close'] = close_price\n",
    "    volume_price = temp_df.iloc[node,4] #volume\n",
    "    G.nodes[node]['volume'] = volume_price\n",
    "\n",
    "\n",
    "label_close = df_close[selected_stocks_final].iloc[rebal_date_index+20,:].values \n",
    "\n",
    "close_price = df_close[selected_stocks_final].iloc[rebal_date_index,:].values\n",
    "\n",
    "\n",
    "# 정답값 만들어주기 #\n",
    "label_df = pd.DataFrame(columns = selected_stocks_final, index = ['label'])\n",
    "label_list = []\n",
    "for stock in range(len(selected_stocks_final)):\n",
    "    if label_close[stock] > close_price[stock]:\n",
    "        label_list.append(1)\n",
    "    else:\n",
    "        label_list.append(0)\n",
    "        \n",
    "\n",
    "label_df.loc['label',:] = label_list #이후에 G의 Label값으로 넣어주기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN으로 hidden 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "label_df = label_df.astype(int)\n",
    "def nx_to_pyg(G, label_df):\n",
    "    # Extracting node features\n",
    "    x = torch.tensor([list(G.nodes[node].values()) for node in G.nodes], dtype=torch.float)\n",
    "    \n",
    "    # Extracting edge indices\n",
    "    edge_index = torch.tensor([[e[0], e[1]] for e in G.edges], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Extracting edge weights\n",
    "    edge_weights = torch.tensor([G[u][v]['weight'] for u, v in G.edges], dtype=torch.float)\n",
    "    \n",
    "    # Extracting node labels\n",
    "    y = torch.tensor(label_df.values[0], dtype=torch.float)\n",
    "    \n",
    "    # Creating PyTorch Geometric data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_weight=edge_weights, y=y)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Assuming G is the NetworkX graph and label_df is the DataFrame containing labels\n",
    "pyg_G = nx_to_pyg(G, label_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check cuda version from torch\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, df_close, df_markets, temp_df, selected_stocks_final):\n",
    "        self.df_close = df_close\n",
    "        self.df_markets = df_markets\n",
    "        self.temp_df = temp_df\n",
    "        self.selected_stocks_final = selected_stocks_final\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_close) - 20  # 이전 시점 데이터를 사용하므로 마지막 20일은 제외\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        rebal_date = self.df_close.index[idx + 20]  # 현재 날짜 (rebal_date)\n",
    "        rebal_date_index = self.df_close.index.get_loc(rebal_date)\n",
    "        \n",
    "        # 현재 날짜의 label\n",
    "        label_close = self.df_close[self.selected_stocks_final].iloc[idx + 20].values\n",
    "        close_price = self.df_close[self.selected_stocks_final].iloc[idx].values\n",
    "        label_list = [1 if label > close else 0 for label, close in zip(label_close, close_price)]\n",
    "        label = torch.tensor(label_list, dtype=torch.float32)\n",
    "        \n",
    "        # 현재 날짜의 그래프 feature\n",
    "        G = get_network_PMFG(corr_matrix)  # 이전과 동일한 방식으로 그래프 생성\n",
    "        for node in G.nodes:\n",
    "            open_price = self.temp_df.iloc[node, 0]  # open\n",
    "            G.nodes[node]['open'] = open_price\n",
    "            high_price = self.temp_df.iloc[node, 1]  # high\n",
    "            G.nodes[node]['high'] = high_price\n",
    "            low_price = self.temp_df.iloc[node, 2]  # low\n",
    "            G.nodes[node]['low'] = low_price\n",
    "            close_price = self.temp_df.iloc[node, 3]  # close\n",
    "            G.nodes[node]['close'] = close_price\n",
    "            volume_price = self.temp_df.iloc[node, 4]  # volume\n",
    "            G.nodes[node]['volume'] = volume_price\n",
    "        \n",
    "        # 이전 시점의 그래프 feature\n",
    "        prev_graphs = []\n",
    "        for i in range(5):\n",
    "            prev_date = self.df_close.index[idx + 20 - i - 1]\n",
    "            prev_temp_df = self.temp_df.loc[:, ['open', 'high', 'low', 'close', 'volume']].loc[prev_date]\n",
    "            prev_graph = get_network_PMFG(corr_matrix, prev_temp_df)\n",
    "            prev_graphs.append(prev_graph)\n",
    "        \n",
    "        return G, prev_graphs, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [00:00<00:00, 10609.91it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Timestamp('2006-08-16 00:00:00')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: Timestamp('2006-08-16 00:00:00')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 모델 학습 (이전과 동일)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m graphs, prev_graphs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     67\u001b[0m         graphs \u001b[38;5;241m=\u001b[39m [g\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m graphs]\n\u001b[0;32m     68\u001b[0m         prev_graphs \u001b[38;5;241m=\u001b[39m [[g\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m prev] \u001b[38;5;28;01mfor\u001b[39;00m prev \u001b[38;5;129;01min\u001b[39;00m prev_graphs]\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m, in \u001b[0;36mStockDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     42\u001b[0m     prev_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_close\u001b[38;5;241m.\u001b[39mindex[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 43\u001b[0m     prev_temp_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhigh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvolume\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprev_date\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     44\u001b[0m     prev_graph \u001b[38;5;241m=\u001b[39m get_network_PMFG(corr_matrix, prev_temp_df)\n\u001b[0;32m     45\u001b[0m     prev_graphs\u001b[38;5;241m.\u001b[39mappend(prev_graph)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\pandas\\core\\indexing.py:1393\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\pandas\\core\\indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\pandas\\core\\generic.py:4236\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4234\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[0;32m   4235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4236\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: Timestamp('2006-08-16 00:00:00')"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# GCN 모델 정의\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_size, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# GCN을 적용한 LSTM 모델 정의\n",
    "class GCNLSTMModel(nn.Module):\n",
    "    def __init__(self, gcn_input_size, gcn_hidden_size, lstm_input_size, lstm_hidden_size, num_layers, num_classes):\n",
    "        super(GCNLSTMModel, self).__init__()\n",
    "        self.gcn_model = GCNModel(gcn_input_size, gcn_hidden_size)\n",
    "        self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, data, prev_data):\n",
    "        gcn_output = self.gcn_model(data)\n",
    "        lstm_input = torch.cat((gcn_output.unsqueeze(0), prev_data), dim=0)\n",
    "        lstm_input = lstm_input.permute(1, 0, 2)  # LSTM의 입력 형태에 맞게 변환\n",
    "        lstm_output, _ = self.lstm(lstm_input)\n",
    "        out = self.fc(lstm_output[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "gcn_input_size = 5  # 각 노드의 feature 개수\n",
    "gcn_hidden_size = 64\n",
    "lstm_input_size = gcn_hidden_size\n",
    "lstm_hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성 (이전과 동일)\n",
    "dataset = StockDataset(df_close, df_markets, temp_df, selected_stocks_final)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 모델 초기화\n",
    "model = GCNLSTMModel(gcn_input_size, gcn_hidden_size, lstm_input_size, lstm_hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# 손실 및 최적화 함수 (이전과 동일)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 학습 (이전과 동일)\n",
    "for epoch in range(num_epochs):\n",
    "    for graphs, prev_graphs, labels in train_loader:\n",
    "        graphs = [g.to(device) for g in graphs]\n",
    "        prev_graphs = [[g.to(device) for g in prev] for prev in prev_graphs]\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 그래프 및 이전 시점 그래프를 GCN 모델에 입력할 수 있는 형태로 변환\n",
    "        data_list = [torch_geometric.data.Data(x=g.nodes[node].values(), edge_index=nx.to_numpy_array(g)) for g in graphs]\n",
    "        prev_data_list = [[[torch.tensor(list(prev_g.nodes[node].values())).unsqueeze(0) for node in prev_g.nodes] for prev_g in prev_graphs_single] for prev_graphs_single in prev_graphs]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data_list, prev_data_list)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        # Backward 및 optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 학습 결과 출력\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('학습 완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "\n",
    "class GCN_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=16, output_dim=2):\n",
    "        super(GCN_LSTM, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.lstm = torch.nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # GCN 실행\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.gelu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # LSTM 입력을 위해 차원 변환\n",
    "        x = x.unsqueeze(0)  # 배치 차원 추가\n",
    "        x, _ = self.lstm(x)  # LSTM 실행\n",
    "        x = x.squeeze(0)  # 배치 차원 제거\n",
    "\n",
    "        # 최종 예측\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "class GCN_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=16, output_dim=2, num_nodes=30):\n",
    "        super(GCN_LSTM, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.lstm = torch.nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # GCN 실행\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.gelu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # 각 시간 단계별로 hidden representation을 저장할 리스트\n",
    "        hidden_representations = []\n",
    "        \n",
    "        # 데이터를 시간 단계별로 분할하여 각 시간 단계에 대한 부분 그래프 생성\n",
    "        for t in range(data.num_time_steps):\n",
    "            # 각 시간 단계에 해당하는 노드들의 인덱스 선택\n",
    "            node_indices = torch.arange(t * self.num_nodes, (t + 1) * self.num_nodes)\n",
    "            \n",
    "            # 해당 시간 단계에 해당하는 부분 그래프 생성\n",
    "            subgraph = Data(x=x[node_indices], edge_index=edge_index)\n",
    "            \n",
    "            # GCN을 통해 부분 그래프에 대한 hidden representation 계산\n",
    "            subgraph_representation = self.conv2(subgraph.x, subgraph.edge_index)\n",
    "            hidden_representations.append(subgraph_representation)\n",
    "\n",
    "        # LSTM의 입력 형태로 hidden representation 변환\n",
    "        hidden_representations = torch.stack(hidden_representations, dim=0)  # (num_time_steps, num_nodes, hidden_dim)\n",
    "        \n",
    "        # LSTM 실행\n",
    "        lstm_out, _ = self.lstm(hidden_representations)\n",
    "\n",
    "        # 최종 예측\n",
    "        final_hidden_state = lstm_out[:, -1, :]  # LSTM의 마지막 시간 단계의 hidden state를 사용\n",
    "        x = self.fc(final_hidden_state)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_LSTM(\n",
      "  (conv1): GCNConv(5, 16)\n",
      "  (conv2): GCNConv(16, 16)\n",
      "  (lstm): LSTM(16, 16, batch_first=True)\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "GCN_LSTM_model = GCN_LSTM()\n",
    "print(GCN_LSTM_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the data to the model\n",
    "output = GCN_LSTM_model(pyg_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongwoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
