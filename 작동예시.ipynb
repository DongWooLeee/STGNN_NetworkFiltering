{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import planarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from Dataset import GraphCustomDataset\n",
    "df_0, df_1, df_2, df_3, df_4, df_5 = load_data()\n",
    "\n",
    "df_list = [df_0, df_1, df_2, df_3, df_4, df_5]\n",
    "\n",
    "df_5.drop(columns=['A060000','A002860'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m GraphCustomDataset(df_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2017-01-03\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2019-02-08\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m, often\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, often_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lee Dong woo\\Desktop\\대한산업공학회\\Dataset.py:80\u001b[0m, in \u001b[0;36mGraphCustomDataset.prepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m graph_sequences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m past_days \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_term)): \u001b[38;5;66;03m#sequence를 담아야 하므로 역순으로 간다. iloc이므로 정수인덱싱. 따라서 \u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_stocks_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrebal_idx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mpast_days\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m     82\u001b[0m         open_price \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_open[selected_stocks_final[node]]\u001b[38;5;241m.\u001b[39miloc[rebal_idx\u001b[38;5;241m-\u001b[39mpast_days]\n",
      "File \u001b[1;32mc:\\Users\\Lee Dong woo\\Desktop\\대한산업공학회\\Dataset.py:70\u001b[0m, in \u001b[0;36mGraphCustomDataset.make_graph\u001b[1;34m(self, selected_stocks_final, ref_idx)\u001b[0m\n\u001b[0;32m     68\u001b[0m input_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_close[selected_stocks_final]\u001b[38;5;241m.\u001b[39miloc[ref_idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_term:ref_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m make_corr(input_df)\n\u001b[1;32m---> 70\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mget_network_PMFG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorr_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[1;32mc:\\Users\\Lee Dong woo\\Desktop\\대한산업공학회\\utils.py:43\u001b[0m, in \u001b[0;36mget_network_PMFG\u001b[1;34m(corr_matrix)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rho, i, j \u001b[38;5;129;01min\u001b[39;00m tqdm(rholist):\n\u001b[0;32m     42\u001b[0m     G\u001b[38;5;241m.\u001b[39madd_edge(i, j, weight\u001b[38;5;241m=\u001b[39mrho)\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mplanarity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_planar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnumber_of_edges() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m (n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     44\u001b[0m         G\u001b[38;5;241m.\u001b[39mremove_edge(i, j)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[1;32mc:\\Users\\Lee Dong woo\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\planarity\\planarity_functions.py:8\u001b[0m, in \u001b[0;36mis_planar\u001b[1;34m(graph)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_planar\u001b[39m(graph):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Test planarity of graph.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplanarity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_planar\u001b[49m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture \n",
    "train_dataset = GraphCustomDataset(df_list, '2017-01-03','2019-02-08', 20, 20, 20, often=True, often_freq = 1)\n",
    "train_dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "valid_dataset = GraphCustomDataset(df_list,'2018-02-09', '2020-12-30',20, 20, 20, often=True, often_freq = 1)\n",
    "valid_dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "test_dataset = GraphCustomDataset(df_list,'2022-01-04', '2022-09-30',20, 20, 20, often=True, often_freq = 20) #실제 리밸런싱 할거니깐. 20일에 한번씩 리밸런싱\n",
    "test_dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 시킬때는 데이터셋이 너무 적으므로, often_freq을 조금 적게 줘서 (즉, 리밸런싱 시점을 다양하게 가져간다) 성능검증 실시. back-testing 에는 이거 뽑는거 자체는 리밸런싱 시점에 대해서만 적용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.9612, Val Loss: 0.7774, Accuracy: 49.46%\n",
      "Validation loss decreased, saving model...\n",
      "Epoch 2, Loss: 2.9827, Val Loss: 0.7046, Accuracy: 49.46%\n",
      "Validation loss decreased, saving model...\n",
      "Epoch 3, Loss: 2.9343, Val Loss: 0.7044, Accuracy: 50.54%\n",
      "Validation loss decreased, saving model...\n",
      "Epoch 4, Loss: 2.9471, Val Loss: 0.7176, Accuracy: 49.46%\n",
      "Epoch 5, Loss: 2.9413, Val Loss: 0.7158, Accuracy: 49.46%\n",
      "Epoch 6, Loss: 2.9407, Val Loss: 0.7158, Accuracy: 49.46%\n",
      "Epoch 7, Loss: 2.8989, Val Loss: 0.6931, Accuracy: 51.06%\n",
      "Validation loss decreased, saving model...\n",
      "Epoch 8, Loss: 2.8996, Val Loss: 0.6931, Accuracy: 51.06%\n",
      "Validation loss decreased, saving model...\n",
      "Epoch 9, Loss: 2.8996, Val Loss: 0.6931, Accuracy: 51.06%\n",
      "Validation loss decreased, saving model...\n",
      "Epoch 10, Loss: 2.8929, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Validation loss decreased, saving model...\n",
      "Epoch 11, Loss: 2.8918, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Epoch 12, Loss: 2.8915, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Epoch 13, Loss: 2.8901, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Epoch 14, Loss: 2.8903, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Epoch 15, Loss: 2.8903, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Epoch 16, Loss: 2.8900, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Epoch 17, Loss: 2.8900, Val Loss: 0.6929, Accuracy: 51.06%\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "#from torch_geometric.nn import GATConv\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "from Dataset import GraphCustomDataset  # Ensure you have this module\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy  # For deep copying the model\n",
    "\n",
    "from model import GCN_LSTM, EarlyStopping\n",
    "\n",
    "#early stopping 구현\n",
    "early_stopping = EarlyStopping(patience=7, verbose=True)\n",
    "\n",
    "# Hyperparameters (adjust batch_size as needed)\n",
    "num_node_features = 5\n",
    "gcn_hidden_dim = 64\n",
    "lstm_hidden_dim = 128\n",
    "num_classes = 2\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 100\n",
    "#batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model, optimizer, and loss function setup\n",
    "model = GCN_LSTM(num_node_features, gcn_hidden_dim, lstm_hidden_dim, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "# Training loop with early stopping and best model checkpointing\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for idx in range(len(train_dataset)):\n",
    "        graphs, labels = train_dataset.get(idx)\n",
    "        graphs = [graph.to(device) for graph in graphs]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)  # Labels as dtype torch.long\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(graphs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(valid_dataset)):\n",
    "            graphs, labels = valid_dataset.get(idx)\n",
    "            graphs = [graph.to(device) for graph in graphs]\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "            outputs = model(graphs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(valid_dataset)\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(valid_dataset):.4f}, Val Loss: {val_loss:.4f}, Accuracy: {100*correct/total:.2f}%')\n",
    "    \n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()  # Learning rate scheduler step\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best model 저장하는 로직, 마코위츠 적용 로직 등 수요일까지 구현. 이후 backtesting 로직 짜기. self.selected_stocks에 있으므로 시점별로 꺼내올 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6923, Accuracy: 58.33%\n"
     ]
    }
   ],
   "source": [
    "# df_close 상에서의 reabl_idx를 가져올 수 있다.\n",
    "from model import GCN_LSTM, EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 그떄 그때 꺼내올 수 있도록 리스트로서 저장\n",
    "num_node_features = 5\n",
    "gcn_hidden_dim = 64\n",
    "lstm_hidden_dim = 128\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model = GCN_LSTM(num_node_features, gcn_hidden_dim, lstm_hidden_dim, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('./checkpoints/checkpoint_model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "#시점별 1로 판단한 stock을 담을 리스트\n",
    "test_prediction_list = []\n",
    "test_stock_list = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(test_dataset)):\n",
    "        graphs, labels = test_dataset.get(idx)\n",
    "        graphs = [graph.to(device) for graph in graphs]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        stocks_list_per_day = test_dataset.stocks_list[idx]\n",
    "        outputs = model(graphs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        test_prediction_list.append(predicted) # 각 stock들에 대한 판단 결과\n",
    "        test_stock_list.append(stocks_list_per_day) # 각 시점별 선정된 stock들 티커모음.\n",
    "        \n",
    "        \n",
    "print(f'Test Loss: {test_loss/len(test_dataset):.4f}, Accuracy: {100*correct/total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A005930',\n",
       " 'A005380',\n",
       " 'A000660',\n",
       " 'A015760',\n",
       " 'A090430',\n",
       " 'A012330',\n",
       " 'A032830',\n",
       " 'A005490',\n",
       " 'A028260',\n",
       " 'A055550',\n",
       " 'A018260',\n",
       " 'A017670',\n",
       " 'A035420',\n",
       " 'A000270',\n",
       " 'A051910',\n",
       " 'A105560',\n",
       " 'A002790',\n",
       " 'A000810',\n",
       " 'A051900',\n",
       " 'A033780',\n",
       " 'A003550',\n",
       " 'A034220',\n",
       " 'A034730',\n",
       " 'A096770',\n",
       " 'A009540',\n",
       " 'A066570',\n",
       " 'A010130',\n",
       " 'A068270',\n",
       " 'A086790',\n",
       " 'A004020']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_list_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_stock_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of stock and its prediction\n",
    "up_list = []\n",
    "down_list = []\n",
    "for i in range(len(test_stock_list)):\n",
    "    stock_pred_dict = {} # 시점별로 stock 별 prediction에 대한 dictionary를 생성\n",
    "    for j in range(len(test_stock_list[i])): # 각 stock 별로 prediction을 dictionary에 저장\n",
    "        stock_pred_dict[test_stock_list[i][j]] = test_prediction_list[i][j].item()\n",
    "    # then make a list of stock that has been predicted as 1 and 0 respectively\n",
    "    stock_pred_1 = []\n",
    "    stock_pred_0 = []\n",
    "    for key, value in stock_pred_dict.items():\n",
    "        if value == 1:\n",
    "            stock_pred_1.append(key)\n",
    "        elif value == 0:\n",
    "            stock_pred_0.append(key)\n",
    "            \n",
    "    up_list.append(stock_pred_1)\n",
    "    down_list.append(stock_pred_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0937,  0.0993],\n",
       "        [ 0.2594, -0.0833],\n",
       "        [ 0.1619,  0.0685],\n",
       "        [ 0.1363,  0.1083],\n",
       "        [ 0.1323,  0.1144],\n",
       "        [ 0.1318,  0.1153],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154],\n",
       "        [ 0.1317,  0.1154]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4519, 0.5481],\n",
       "        [0.5848, 0.4152],\n",
       "        [0.5233, 0.4767],\n",
       "        [0.5070, 0.4930],\n",
       "        [0.5045, 0.4955],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959],\n",
       "        [0.5041, 0.4959]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax to the output\n",
    "import torch.nn.functional as F\n",
    "outputs = F.softmax(outputs, dim=1)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PORTFOLIO 구축."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongwoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
